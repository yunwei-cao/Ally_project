{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#About\n",
        "This notebook is part of the summary of my field experience supported by Ally Financial Company, as well as a nice chance to practice what I learnt from the course STOR445 Stochastic Modeling in the semester Spring2023.\n",
        "\n",
        "I came across a post on HN and saw a great intro of GPT as a finite-state markov chain. The reviews below are also inspiring. Here are the related links:\n",
        "\n",
        "https://colab.research.google.com/drive/1SiF0KZJp75rUeetKOWqpsA8clmHP6jMg?usp=sharing#scrollTo=mGHwSuHQuTXI\n",
        "\n",
        "https://news.ycombinator.com/item?id=35506069\n",
        "\n",
        "What in below aims to see what MC looks like when both vocab size and content length are 2, as well as get familiar with the code.\n"
      ],
      "metadata": {
        "id": "5FgeJUiNtNNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcyRzTo1oR3p"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for our GPT\n",
        "\n",
        "# vocab size is 2, so we only have two possible tokens: 0,1\n",
        "vocab_size = 2\n",
        "# context length is 2, so we take 2 bits to predict the next bit probability\n",
        "context_length = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('state space (for this exercise) = ', vocab_size ** context_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7piA9oHoaO-",
        "outputId": "67192c9d-7473-4390-ef89-9a53e9eb507b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state space (for this exercise) =  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('actual state space (in reality) = ', sum(vocab_size ** i for i in range(1, context_length+1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsAMcyJtocjG",
        "outputId": "0dd8e23f-f2fa-478c-fe49-b9a2f6105f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual state space (in reality) =  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the GPT neural net is a sequence of tokens of length\n",
        "`context_length`. These tokens are discrete, so the state space is simply:(00) (01) (10) (11).\n",
        "\n",
        "If the context length is 2, we could in principle feed in 1 token or 2 tokens when trying to predict the next token. Here we are going to ignore this and assume that the context length is \"maxed out\" (which means we just talk about 2 tokens), just to simplify some of the code below, but this is worth keeping in mind."
      ],
      "metadata": {
        "id": "3CVMRCS7x-3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title minimal GPT implementation in PyTorch (optional)\n",
        "\"\"\" super minimal decoder-only gpt \"\"\"\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                    .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # manual implementation of attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.nonlin = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.nonlin(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    # these are default GPT-2 hyperparameters\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    bias: bool = False\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %d\" % (sum(p.nelement() for p in self.parameters()),))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x[:, -1, :]) # note: only returning logits at the last time step (-1), output is 2D (b, vocab_size)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CRqY-CE4oiW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(\n",
        "    block_size = context_length,\n",
        "    vocab_size = vocab_size,\n",
        "    n_layer = 4,\n",
        "    n_head = 4,\n",
        "    n_embd = 16,\n",
        "    bias = False,\n",
        ")\n",
        "gpt = GPT(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-gDCOcvomue",
        "outputId": "b1184cbc-b266-41b4-807e-2440bec3f31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 12640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_possible(n, k):\n",
        "    # return all possible lists of k elements, each in range of [0,n)\n",
        "    if k == 0:\n",
        "        yield []\n",
        "    else:\n",
        "        for i in range(n):\n",
        "            for c in all_possible(n, k - 1):\n",
        "                yield [i] + c\n",
        "list(all_possible(vocab_size, context_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp7v-6Npos8o",
        "outputId": "15881ef5-b0a3-42d6-9831-8201c51077e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0], [0, 1], [1, 0], [1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use graphviz for pretty plotting the current state of the GPT\n",
        "from graphviz import Digraph\n",
        "\n",
        "def plot_model():\n",
        "    dot = Digraph(comment='Baby GPT', engine='circo')\n",
        "\n",
        "    for xi in all_possible(gpt.config.vocab_size, gpt.config.block_size):\n",
        "\n",
        "        # forward the GPT and get probabilities for next token\n",
        "        x = torch.tensor(xi, dtype=torch.long)[None, ...] # turn the list into a torch tensor and add a batch dimension\n",
        "        logits = gpt(x) # forward the gpt neural net\n",
        "        probs = nn.functional.softmax(logits, dim=-1) # get the probabilities\n",
        "        y = probs[0].tolist() # remove the batch dimension and unpack the tensor into simple list\n",
        "        print(f\"input {xi} ---> {y}\")\n",
        "\n",
        "        # also build up the transition graph for plotting later\n",
        "        current_node_signature = \"\".join(str(d) for d in xi)\n",
        "        dot.node(current_node_signature)\n",
        "        for t in range(gpt.config.vocab_size):\n",
        "            next_node = xi[1:] + [t] # crop the context and append the next character\n",
        "            next_node_signature = \"\".join(str(d) for d in next_node)\n",
        "            p = y[t]\n",
        "            label=f\"{t}({p*100:.0f}%)\"\n",
        "            dot.edge(current_node_signature, next_node_signature, label=label)\n",
        "\n",
        "    return dot\n",
        "\n",
        "plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "y7i7v8XhoxGY",
        "outputId": "2e870ad7-4de4-4cca-9d2f-4ab91fe04b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input [0, 0] ---> [0.5834338068962097, 0.4165661931037903]\n",
            "input [0, 1] ---> [0.4045574963092804, 0.5954424738883972]\n",
            "input [1, 0] ---> [0.5810206532478333, 0.418979287147522]\n",
            "input [1, 1] ---> [0.40443000197410583, 0.5955699682235718]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"222pt\" height=\"204pt\"\n viewBox=\"0.00 0.00 222.43 204.43\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 200.43)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-200.43 218.43,-200.43 218.43,4 -4,4\"/>\n<!-- 00 -->\n<g id=\"node1\" class=\"node\">\n<title>00</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"107.21\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"107.21\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">00</text>\n</g>\n<!-- 00&#45;&gt;00 -->\n<g id=\"edge1\" class=\"edge\">\n<title>00&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M132.66,-24.69C143.24,-25.15 152.21,-22.92 152.21,-18 152.21,-14.77 148.35,-12.7 142.7,-11.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"142.81,-8.29 132.66,-11.31 142.48,-15.28 142.81,-8.29\"/>\n<text text-anchor=\"middle\" x=\"173.21\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">0(58%)</text>\n</g>\n<!-- 01 -->\n<g id=\"node2\" class=\"node\">\n<title>01</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-98.21\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-94.51\" font-family=\"Times,serif\" font-size=\"14.00\">01</text>\n</g>\n<!-- 00&#45;&gt;01 -->\n<g id=\"edge2\" class=\"edge\">\n<title>00&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M92.13,-33.09C80.03,-45.19 62.87,-62.35 49.2,-76.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"46.62,-73.65 42.02,-83.19 51.57,-78.6 46.62,-73.65\"/>\n<text text-anchor=\"middle\" x=\"49.66\" y=\"-58.35\" font-family=\"Times,serif\" font-size=\"14.00\">1(42%)</text>\n</g>\n<!-- 10 -->\n<g id=\"node3\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"187.43\" cy=\"-98.21\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"187.43\" y=\"-94.51\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n</g>\n<!-- 01&#45;&gt;10 -->\n<g id=\"edge3\" class=\"edge\">\n<title>01&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M53.13,-103.35C79.82,-105.27 121.45,-105.48 151.29,-103.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.54,-107.45 161.31,-103.35 151.11,-100.47 151.54,-107.45\"/>\n<text text-anchor=\"middle\" x=\"81.21\" y=\"-107.46\" font-family=\"Times,serif\" font-size=\"14.00\">0(40%)</text>\n</g>\n<!-- 11 -->\n<g id=\"node4\" class=\"node\">\n<title>11</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"107.21\" cy=\"-178.43\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"107.21\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">11</text>\n</g>\n<!-- 01&#45;&gt;11 -->\n<g id=\"edge4\" class=\"edge\">\n<title>01&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M42.09,-113.3C54.19,-125.4 71.35,-142.56 85.01,-156.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"82.65,-158.81 92.19,-163.41 87.6,-153.86 82.65,-158.81\"/>\n<text text-anchor=\"middle\" x=\"42.55\" y=\"-138.57\" font-family=\"Times,serif\" font-size=\"14.00\">1(60%)</text>\n</g>\n<!-- 10&#45;&gt;00 -->\n<g id=\"edge5\" class=\"edge\">\n<title>10&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.34,-83.13C160.24,-71.03 143.08,-53.87 129.41,-40.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.78,-37.62 122.23,-33.02 126.83,-42.57 131.78,-37.62\"/>\n<text text-anchor=\"middle\" x=\"129.88\" y=\"-65.46\" font-family=\"Times,serif\" font-size=\"14.00\">0(58%)</text>\n</g>\n<!-- 10&#45;&gt;01 -->\n<g id=\"edge6\" class=\"edge\">\n<title>10&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M161.3,-93.07C134.61,-91.15 92.97,-90.95 63.14,-92.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"62.89,-88.98 53.12,-93.07 63.32,-95.96 62.89,-88.98\"/>\n<text text-anchor=\"middle\" x=\"91.22\" y=\"-81.57\" font-family=\"Times,serif\" font-size=\"14.00\">1(42%)</text>\n</g>\n<!-- 11&#45;&gt;10 -->\n<g id=\"edge7\" class=\"edge\">\n<title>11&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M122.3,-163.34C134.4,-151.24 151.56,-134.08 165.23,-120.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167.81,-122.78 172.41,-113.23 162.86,-117.83 167.81,-122.78\"/>\n<text text-anchor=\"middle\" x=\"122.77\" y=\"-145.68\" font-family=\"Times,serif\" font-size=\"14.00\">0(40%)</text>\n</g>\n<!-- 11&#45;&gt;11 -->\n<g id=\"edge8\" class=\"edge\">\n<title>11&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M132.66,-185.12C143.24,-185.58 152.21,-183.35 152.21,-178.43 152.21,-175.2 148.35,-173.13 142.7,-172.22\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"142.81,-168.72 132.66,-171.74 142.48,-175.71 142.81,-168.72\"/>\n<text text-anchor=\"middle\" x=\"173.21\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">1(60%)</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fbe40099c70>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see our 4 states, and the probabilistic arrows that connect them. Because there are 2 possible tokens, there are 2 possible arrows coming out of each node. Note that every time we \"transition\" via an edge, the leftmost token gets dropped, and the token on that edge gets appended to the right. Notice that at initialization, most of these probabilities are around uniform (50% in this case), which is nice and desirable, as we haven't even trained the model at all."
      ],
      "metadata": {
        "id": "vlXZPgO9zP6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's train our baby GPT on this sequence\n",
        "seq = list(map(int, \"111101111011110\"))\n",
        "seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79Xhdvho26L",
        "outputId": "0d7d7ec2-88d2-4951-e834-a3b6559936b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the sequence to a tensor holding all the individual examples in that sequence\n",
        "X, Y = [], []\n",
        "# iterate over the sequence and grab every consecutive 3 bits\n",
        "# the correct label for what's next is the next bit at each position\n",
        "for i in range(len(seq) - context_length):\n",
        "    X.append(seq[i:i+context_length])\n",
        "    Y.append(seq[i+context_length])\n",
        "    print(f\"example {i+1:2d}: {X[-1]} --> {Y[-1]}\")\n",
        "X = torch.tensor(X, dtype=torch.long)\n",
        "Y = torch.tensor(Y, dtype=torch.long)\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCqjwX6do4UI",
        "outputId": "8b562187-1096-4c80-82a0-a87e63b1d920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example  1: [1, 1] --> 1\n",
            "example  2: [1, 1] --> 1\n",
            "example  3: [1, 1] --> 0\n",
            "example  4: [1, 0] --> 1\n",
            "example  5: [0, 1] --> 1\n",
            "example  6: [1, 1] --> 1\n",
            "example  7: [1, 1] --> 1\n",
            "example  8: [1, 1] --> 0\n",
            "example  9: [1, 0] --> 1\n",
            "example 10: [0, 1] --> 1\n",
            "example 11: [1, 1] --> 1\n",
            "example 12: [1, 1] --> 1\n",
            "example 13: [1, 1] --> 0\n",
            "torch.Size([13, 2]) torch.Size([13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init a GPT and the optimizer\n",
        "torch.manual_seed(1337)\n",
        "gpt = GPT(config)\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3, weight_decay=1e-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vij4iBmvo-e2",
        "outputId": "1ba072cf-f0a0-4af0-c972-4c12cdfebdb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 12640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the GPT for some number of iterations\n",
        "for i in range(50):\n",
        "    logits = gpt(X)\n",
        "    loss = F.cross_entropy(logits, Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(i, loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o4TTYx4o_lV",
        "outputId": "384a03bf-948f-477c-edad-c12aa7ebdf27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.6616935133934021\n",
            "1 0.6317402124404907\n",
            "2 0.6050006747245789\n",
            "3 0.5929291248321533\n",
            "4 0.5903751254081726\n",
            "5 0.587951123714447\n",
            "6 0.5842908620834351\n",
            "7 0.5807065367698669\n",
            "8 0.577465295791626\n",
            "9 0.5744820833206177\n",
            "10 0.571670651435852\n",
            "11 0.5689849257469177\n",
            "12 0.5664036273956299\n",
            "13 0.5639188885688782\n",
            "14 0.5615299344062805\n",
            "15 0.5592407584190369\n",
            "16 0.5570589900016785\n",
            "17 0.5549943447113037\n",
            "18 0.5530572533607483\n",
            "19 0.5512576699256897\n",
            "20 0.5496029257774353\n",
            "21 0.5480983257293701\n",
            "22 0.5467461943626404\n",
            "23 0.5455450415611267\n",
            "24 0.5444915890693665\n",
            "25 0.5435795187950134\n",
            "26 0.5428012609481812\n",
            "27 0.5421478152275085\n",
            "28 0.5416089296340942\n",
            "29 0.5411741733551025\n",
            "30 0.5408323407173157\n",
            "31 0.5405718684196472\n",
            "32 0.5403817296028137\n",
            "33 0.5402498841285706\n",
            "34 0.5401651859283447\n",
            "35 0.540116012096405\n",
            "36 0.5400907397270203\n",
            "37 0.5400779843330383\n",
            "38 0.5400656461715698\n",
            "39 0.5400412082672119\n",
            "40 0.5399907231330872\n",
            "41 0.5398979783058167\n",
            "42 0.539743185043335\n",
            "43 0.5394999980926514\n",
            "44 0.5391327142715454\n",
            "45 0.5385890007019043\n",
            "46 0.5377923250198364\n",
            "47 0.5366281867027283\n",
            "48 0.5349264144897461\n",
            "49 0.5324529409408569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data sequence, as a reminder:\", seq)\n",
        "plot_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "6aT_QnMGpB9q",
        "outputId": "d8ccb6a2-4c26-4ac4-da72-962b92e9e17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data sequence, as a reminder: [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n",
            "input [0, 0] ---> [0.21870099008083344, 0.7812989950180054]\n",
            "input [0, 1] ---> [0.2317209243774414, 0.7682790756225586]\n",
            "input [1, 0] ---> [0.21896907687187195, 0.7810309529304504]\n",
            "input [1, 1] ---> [0.25789374113082886, 0.7421062588691711]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"222pt\" height=\"204pt\"\n viewBox=\"0.00 0.00 222.43 204.43\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 200.43)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-200.43 218.43,-200.43 218.43,4 -4,4\"/>\n<!-- 00 -->\n<g id=\"node1\" class=\"node\">\n<title>00</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"107.21\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"107.21\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">00</text>\n</g>\n<!-- 00&#45;&gt;00 -->\n<g id=\"edge1\" class=\"edge\">\n<title>00&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M132.66,-24.69C143.24,-25.15 152.21,-22.92 152.21,-18 152.21,-14.77 148.35,-12.7 142.7,-11.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"142.81,-8.29 132.66,-11.31 142.48,-15.28 142.81,-8.29\"/>\n<text text-anchor=\"middle\" x=\"173.21\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">0(22%)</text>\n</g>\n<!-- 01 -->\n<g id=\"node2\" class=\"node\">\n<title>01</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-98.21\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-94.51\" font-family=\"Times,serif\" font-size=\"14.00\">01</text>\n</g>\n<!-- 00&#45;&gt;01 -->\n<g id=\"edge2\" class=\"edge\">\n<title>00&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M92.13,-33.09C80.03,-45.19 62.87,-62.35 49.2,-76.01\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"46.62,-73.65 42.02,-83.19 51.57,-78.6 46.62,-73.65\"/>\n<text text-anchor=\"middle\" x=\"49.66\" y=\"-58.35\" font-family=\"Times,serif\" font-size=\"14.00\">1(78%)</text>\n</g>\n<!-- 10 -->\n<g id=\"node3\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"187.43\" cy=\"-98.21\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"187.43\" y=\"-94.51\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n</g>\n<!-- 01&#45;&gt;10 -->\n<g id=\"edge3\" class=\"edge\">\n<title>01&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M53.13,-103.35C79.82,-105.27 121.45,-105.48 151.29,-103.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"151.54,-107.45 161.31,-103.35 151.11,-100.47 151.54,-107.45\"/>\n<text text-anchor=\"middle\" x=\"81.21\" y=\"-107.46\" font-family=\"Times,serif\" font-size=\"14.00\">0(23%)</text>\n</g>\n<!-- 11 -->\n<g id=\"node4\" class=\"node\">\n<title>11</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"107.21\" cy=\"-178.43\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"107.21\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">11</text>\n</g>\n<!-- 01&#45;&gt;11 -->\n<g id=\"edge4\" class=\"edge\">\n<title>01&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M42.09,-113.3C54.19,-125.4 71.35,-142.56 85.01,-156.23\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"82.65,-158.81 92.19,-163.41 87.6,-153.86 82.65,-158.81\"/>\n<text text-anchor=\"middle\" x=\"42.55\" y=\"-138.57\" font-family=\"Times,serif\" font-size=\"14.00\">1(77%)</text>\n</g>\n<!-- 10&#45;&gt;00 -->\n<g id=\"edge5\" class=\"edge\">\n<title>10&#45;&gt;00</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.34,-83.13C160.24,-71.03 143.08,-53.87 129.41,-40.2\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"131.78,-37.62 122.23,-33.02 126.83,-42.57 131.78,-37.62\"/>\n<text text-anchor=\"middle\" x=\"129.88\" y=\"-65.46\" font-family=\"Times,serif\" font-size=\"14.00\">0(22%)</text>\n</g>\n<!-- 10&#45;&gt;01 -->\n<g id=\"edge6\" class=\"edge\">\n<title>10&#45;&gt;01</title>\n<path fill=\"none\" stroke=\"black\" d=\"M161.3,-93.07C134.61,-91.15 92.97,-90.95 63.14,-92.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"62.89,-88.98 53.12,-93.07 63.32,-95.96 62.89,-88.98\"/>\n<text text-anchor=\"middle\" x=\"91.22\" y=\"-81.57\" font-family=\"Times,serif\" font-size=\"14.00\">1(78%)</text>\n</g>\n<!-- 11&#45;&gt;10 -->\n<g id=\"edge7\" class=\"edge\">\n<title>11&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M122.3,-163.34C134.4,-151.24 151.56,-134.08 165.23,-120.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167.81,-122.78 172.41,-113.23 162.86,-117.83 167.81,-122.78\"/>\n<text text-anchor=\"middle\" x=\"122.77\" y=\"-145.68\" font-family=\"Times,serif\" font-size=\"14.00\">0(26%)</text>\n</g>\n<!-- 11&#45;&gt;11 -->\n<g id=\"edge8\" class=\"edge\">\n<title>11&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M132.66,-185.12C143.24,-185.58 152.21,-183.35 152.21,-178.43 152.21,-175.2 148.35,-173.13 142.7,-172.22\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"142.81,-168.72 132.66,-171.74 142.48,-175.71 142.81,-168.72\"/>\n<text text-anchor=\"middle\" x=\"173.21\" y=\"-174.73\" font-family=\"Times,serif\" font-size=\"14.00\">1(74%)</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7fbe4004d550>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The arrows that correspond to transitions in our training data get higher probabilities. That makes sense. For example:\n",
        "\n",
        "In our training data 10 always transitions to 01. After 50 steps of training, we see that this transition has 78% probability. Inversely 01 hardly transfer to 10 in the training data so the prob is relatively low (23%).\n",
        "\n",
        "The states 00 that never appeared in the training data has substantial probabilities(22%) for what tokens should come next. This is desirable because in a real application scenario during deployment, almost every test input to the GPT is a never-before-seen input during training. We rely on the internals of the GPT (and its \"inductive bias\") to perform the generalization appropriately.\n",
        "\n"
      ],
      "metadata": {
        "id": "_DZkuYe00Nur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xi = [1, 1] # the starting sequence\n",
        "fullseq = xi.copy()\n",
        "print(f\"init: {xi}\")\n",
        "for k in range(20):\n",
        "    x = torch.tensor(xi, dtype=torch.long)[None, ...]\n",
        "    logits = gpt(x)\n",
        "    probs = nn.functional.softmax(logits, dim=-1)\n",
        "    t = torch.multinomial(probs[0], num_samples=1).item() # sample from the probability distribution\n",
        "    xi = xi[1:] + [t] # transition to the next state\n",
        "    fullseq.append(t)\n",
        "    print(f\"step {k}: state {xi}\")\n",
        "\n",
        "print(\"\\nfull sampled sequence:\")\n",
        "print(\"\".join(map(str, fullseq)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMakObiZpNLA",
        "outputId": "4bdfd1af-7670-48f8-ff6f-e97dd17bf1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "init: [1, 1]\n",
            "step 0: state [1, 1]\n",
            "step 1: state [1, 1]\n",
            "step 2: state [1, 1]\n",
            "step 3: state [1, 1]\n",
            "step 4: state [1, 1]\n",
            "step 5: state [1, 1]\n",
            "step 6: state [1, 1]\n",
            "step 7: state [1, 0]\n",
            "step 8: state [0, 0]\n",
            "step 9: state [0, 1]\n",
            "step 10: state [1, 0]\n",
            "step 11: state [0, 1]\n",
            "step 12: state [1, 1]\n",
            "step 13: state [1, 1]\n",
            "step 14: state [1, 0]\n",
            "step 15: state [0, 0]\n",
            "step 16: state [0, 1]\n",
            "step 17: state [1, 1]\n",
            "step 18: state [1, 1]\n",
            "step 19: state [1, 1]\n",
            "\n",
            "full sampled sequence:\n",
            "1111111110010111001111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on how much you train your network, these sequences will look more and more like the training data. In our case we'd never get a perfect match because the state 11 has an ambiguous future: 50% of the time it's 1, 50% time a 0."
      ],
      "metadata": {
        "id": "O0BaeOoa1lVq"
      }
    }
  ]
}